# ğŸŒ¦ï¸ Airflow Weather ETL Pipeline

An end-to-end **ETL pipeline** built using **Apache Airflow**, **PostgreSQL**, and **Docker**.  
The pipeline fetches live weather data from the Weatherstack API, cleans and validates it, and stores it in a PostgreSQL database for further analytics.

### ğŸ“Š ETL Pipeline Diagram

![ETL Pipeline](assets/etl_pipeline.png)
---

## ğŸš€ Features

âœ… Automated data ingestion using Airflow  
âœ… Data cleaning & validation in Python  
âœ… Raw + cleaned data stored in PostgreSQL  
âœ… Fully containerized with Docker  
âœ… Idempotent inserts (no duplicate rows)  
âœ… Organized project structure (ETL scripts + DAGs)  
âœ… Environment variables for secrets (no API keys in repo)

---

## ğŸ—ï¸ Architecture

Weather API â†’ Extract (Python) â†’ Clean â†’ Load â†’ PostgreSQL
â”‚
â†“
Apache Airflow (DAG)
---

## ğŸ“¦ Tech Stack

| Component | Technology |
|-----------|------------|
| Workflow Orchestration | Apache Airflow |
| Database | PostgreSQL |
| Containerization | Docker & Docker Compose |
| Language | Python |
| API Source | Weatherstack API |

---

## ğŸ§° Project Structure
/
â”œâ”€ airflow/
â”‚ â”œâ”€ dags/
â”‚ â”‚ â””â”€ orchestrator.py
â”œâ”€ api_request/
â”‚ â”œâ”€ api_request.py
â”‚ â”œâ”€ insert_records.py
â”œâ”€ postgres/
â”‚ â””â”€ init-airflow.sql
â”œâ”€ docker-compose.yml
â”œâ”€ .env.example
â”œâ”€ README.md

---

## âš™ï¸ Setup & Run

### 1ï¸âƒ£ Clone the repo
### 2ï¸âƒ£ Create a .env file based on .env.example
Add your API key and DB credentials.
### 3ï¸âƒ£ Start the project
docker-compose up -d
### 4ï¸âƒ£ Open Airflow UI
http://localhost:8080
### 5ï¸âƒ£ Trigger the DAG: weather_etl_dag
